#!/usr/bin/env python3
"""
ç”µè§†ç›´æ’­æºæ”¶é›†è„šæœ¬ - ç®€åŒ–ç‰ˆï¼ˆå•æ–‡ä»¶ç‰ˆæœ¬ï¼‰
ç”¨äºæ¨¡å—åŒ–ç‰ˆæœ¬å¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆ
"""
import requests
import re
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
import json
import os
import sys
import ipaddress
import concurrent.futures

print("=" * 70)
print("ç”µè§†ç›´æ’­æºæ”¶é›†è„šæœ¬ v7.0 - ç®€åŒ–ç‰ˆï¼ˆå•æ–‡ä»¶ï¼‰")
print("åŠŸèƒ½ï¼šé¢‘é“åç§°ç²¾ç®€ã€åŒåç”µè§†å°åˆå¹¶ã€IPv6ä¼˜å…ˆæ’åºã€æ…¢é€Ÿæºé»‘åå•è¿‡æ»¤")
print("ç‰¹ç‚¹ï¼šæ¯ä¸ªç”µè§†å°æ˜¾ç¤ºä¸ºä¸€ä¸ªæ¡ç›®ï¼ŒIPv6æºä¼˜å…ˆæ’åˆ—")
print("=" * 70)

# åŸºæœ¬é…ç½®
BLACKLIST_FILE = "blacklist.txt"
SPEED_TEST_TIMEOUT = 6
MAX_WORKERS = 20

def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    utc_now = datetime.now(timezone.utc)
    beijing_time = utc_now.astimezone(timezone(timedelta(hours=8)))
    return beijing_time.strftime('%Y-%m-%d %H:%M:%S')

def load_sources_from_file():
    """åŠ è½½æ•°æ®æº"""
    sources_file = "sources.txt"
    sources = []
    
    if not os.path.exists(sources_file):
        print(f"âŒ é”™è¯¯: {sources_file} æ–‡ä»¶ä¸å­˜åœ¨")
        return sources
    
    try:
        with open(sources_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and (line.startswith("http://") or line.startswith("https://")):
                    sources.append(line)
        
        print(f"ğŸ“¡ ä» {sources_file} åŠ è½½äº† {len(sources)} ä¸ªæ•°æ®æº")
        
    except Exception as e:
        print(f"âŒ è¯»å– {sources_file} å¤±è´¥: {e}")
    
    return sources

def fetch_m3u(url, retry=2):
    """è·å–M3Uæ–‡ä»¶"""
    for attempt in range(retry + 1):
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/plain,application/x-mpegURL,*/*",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Connection": "keep-alive",
                "Cache-Control": "no-cache"
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                return response.text
            else:
                print(f"âŒ è·å–å¤±è´¥ {url}: HTTP {response.status_code}")
                if attempt < retry:
                    time.sleep(2)
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚é”™è¯¯ {url}: {e}")
            if attempt < retry:
                time.sleep(2)
    
    return None

def clean_channel_name(name):
    """æ¸…ç†é¢‘é“åç§°"""
    # ç®€å•çš„æ¸…ç†è§„åˆ™
    clean_rules = [
        (r'[\[\(][^\]\)]*[\]\)]', ''),
        (r'ã€[^ã€‘]*ã€‘', ''),
        (r'\s+ç›´æ’­$', ''),
        (r'\s+é¢‘é“$', ''),
        (r'\s+å°$', ''),
        (r'\s+ç”µè§†å°$', ''),
        (r'[_\-\|]+', ' '),
        (r'\s+', ' '),
    ]
    
    for pattern, replacement in clean_rules:
        name = re.sub(pattern, replacement, name)
    
    # æ ‡å‡†åŒ–CCTV
    if 'cctv' in name.lower():
        name = re.sub(r'cctv', 'CCTV', name, flags=re.IGNORECASE)
    
    name = name.strip()
    return name

def parse_channels(content, source_url):
    """è§£æM3Uå†…å®¹"""
    channels = []
    lines = content.split('\n')
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('#EXTINF:'):
            # æå–é¢‘é“åç§°
            name = "æœªçŸ¥é¢‘é“"
            match = re.search(r',([^,\n]+)$', line)
            if match:
                name = match.group(1).strip()
            
            # è·å–URL
            if i + 1 < len(lines):
                url = lines[i + 1].strip()
                if url and not url.startswith('#'):
                    clean_name = clean_channel_name(name)
                    
                    channels.append({
                        'original_name': name,
                        'clean_name': clean_name,
                        'url': url,
                        'source': source_url
                    })
                    i += 1
        i += 1
    
    return channels

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½æ•°æ®æº
    sources = load_sources_from_file()
    
    if len(sources) == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼Œé€€å‡º")
        sys.exit(1)
    
    # æ”¶é›†é¢‘é“æ•°æ®
    all_channels = []
    success_sources = 0
    failed_sources = []
    
    print("\nğŸ“¡ å¼€å§‹æ”¶é›†é¢‘é“æ•°æ®...")
    for idx, source_url in enumerate(sources, 1):
        print(f"\n[{idx}/{len(sources)}] å¤„ç†: {source_url}")
        
        content = fetch_m3u(source_url)
        if not content:
            failed_sources.append(source_url)
            print("   âŒ æ— æ³•è·å–å†…å®¹ï¼Œè·³è¿‡")
            continue
        
        channels = parse_channels(content, source_url)
        print(f"   âœ… è§£æåˆ° {len(channels)} ä¸ªé¢‘é“")
        
        all_channels.extend(channels)
        success_sources += 1
        
        time.sleep(1)
    
    print(f"\nâœ… é‡‡é›†å®Œæˆ:")
    print(f"   æˆåŠŸæºæ•°: {success_sources}/{len(sources)}")
    print(f"   æ€»è®¡é‡‡é›†: {len(all_channels)} ä¸ªåŸå§‹é¢‘é“")
    
    if len(all_channels) == 0:
        print("âŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•é¢‘é“ï¼Œé€€å‡º")
        sys.exit(1)
    
    # åˆå¹¶åŒåé¢‘é“
    print("\nğŸ”„ æ­£åœ¨åˆå¹¶åŒåç”µè§†å°...")
    merged_channels = {}
    
    for channel in all_channels:
        key = channel['clean_name']
        
        if key not in merged_channels:
            merged_channels[key] = {
                'clean_name': key,
                'sources': [{
                    'url': channel['url'],
                    'source': channel['source']
                }]
            }
        else:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            urls = [s['url'] for s in merged_channels[key]['sources']]
            if channel['url'] not in urls:
                merged_channels[key]['sources'].append({
                    'url': channel['url'],
                    'source': channel['source']
                })
    
    print(f"   åˆå¹¶å: {len(merged_channels)} ä¸ªå”¯ä¸€ç”µè§†å°")
    
    # ç”ŸæˆM3Uæ–‡ä»¶
    timestamp = get_beijing_time()
    
    # åˆ›å»ºç›®å½•
    Path("merged").mkdir(exist_ok=True)
    Path("categories").mkdir(exist_ok=True)
    
    # ç”Ÿæˆä¸»æ–‡ä»¶
    print("\nğŸ“„ ç”Ÿæˆ live_sources.m3u...")
    try:
        with open("live_sources.m3u", "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            f.write(f"# ç”µè§†ç›´æ’­æº - ç®€åŒ–ç‰ˆ\n")
            f.write(f"# æ›´æ–°æ—¶é—´(åŒ—äº¬æ—¶é—´): {timestamp}\n")
            f.write(f"# ç”µè§†å°æ€»æ•°: {len(merged_channels)}\n")
            f.write(f"# æ•°æ®æº: {len(sources)} ä¸ª\n\n")
            
            for channel_name, channel_data in sorted(merged_channels.items()):
                source_count = len(channel_data['sources'])
                
                if source_count > 1:
                    display_name = f"{channel_name} [{source_count}æº]"
                else:
                    display_name = channel_name
                
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæº
                main_url = channel_data['sources'][0]['url']
                
                f.write(f'#EXTINF:-1 tvg-name="{channel_name}" group-title="æ‰€æœ‰é¢‘é“",{display_name}\n')
                f.write(f"{main_url}\n")
        
        print(f"  âœ… live_sources.m3u ç”ŸæˆæˆåŠŸ")
        
        # ç”Ÿæˆç®€å•ç»Ÿè®¡
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  - ç”µè§†å°æ€»æ•°: {len(merged_channels)}")
        print(f"  - å¤šæºç”µè§†å°: {sum(1 for c in merged_channels.values() if len(c['sources']) > 1)}")
        print(f"  - åŸå§‹é¢‘é“æ•°: {len(all_channels)}")
        print(f"  - æ•°æ®æº: {len(sources)}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()