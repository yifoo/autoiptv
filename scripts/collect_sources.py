#!/usr/bin/env python3
"""
è‡ªåŠ¨é‡‡é›†å¹¶å½’ç±»ç”µè§†ç›´æ’­æº
"""

import requests
import re
import time
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import hashlib
from urllib.parse import urlparse
import json

# è¦é‡‡é›†çš„æºåˆ—è¡¨
SOURCES = [
    "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/ipv6.m3u",
    "https://raw.githubusercontent.com/chao921125/source/refs/heads/main/iptv/index.m3u"
]

# å¯ä»¥æ·»åŠ æ›´å¤šæº
ADDITIONAL_SOURCES_FILE = "sources.txt"

# åˆ†ç±»è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é¢‘é“åç§°ï¼‰
CATEGORY_RULES = {
    "å¤®è§†": [
        r"CCTV[-_\s]?\d+", r"CCTV[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+",
        r"å¤®è§†[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+", r"ä¸­å¤®ç”µè§†å°"
    ],
    "å«è§†": [
        r"å«è§†", r"æ¹–å—å«è§†", r"æµ™æ±Ÿå«è§†", r"æ±Ÿè‹å«è§†", r"ä¸œæ–¹å«è§†",
        r"åŒ—äº¬å«è§†", r"å¤©æ´¥å«è§†", r"å®‰å¾½å«è§†", r"å±±ä¸œå«è§†", r"å¹¿ä¸œå«è§†"
    ],
    "åœ°æ–¹å°": [
        r"åœ°æ–¹", r"éƒ½å¸‚", r"æ°‘ç”Ÿ", r"æ–°é—»", r"å…¬å…±", r"ç”Ÿæ´»",
        r"æ•™è‚²", r"å°‘å„¿", r"ç»¼è‰º"
    ],
    "æ¸¯æ¾³å°": [
        r"å‡¤å‡°", r"ç¿¡ç¿ ", r"æ˜ç ", r"TVB", r"ATV", r"æ¾³è§†",
        r"æ¾³é—¨", r"é¦™æ¸¯", r"å°æ¹¾", r"ä¸­å¤©", r"ä¸œæ£®"
    ],
    "ä½“è‚²": [
        r"ä½“è‚²", r"NBA", r"è¶³çƒ", r"ç¯®çƒ", r"é«˜å°”å¤«", r"ç½‘çƒ"
    ],
    "ç”µå½±": [
        r"ç”µå½±", r"å½±é™¢", r"å½±è§†é¢‘é“"
    ],
    "éŸ³ä¹": [
        r"éŸ³ä¹", r"MTV", r"æ¼”å”±ä¼š"
    ]
}

class Channel:
    """é¢‘é“ç±»"""
    def __init__(self, name, url, group=None, logo=None):
        self.name = name.strip()
        self.url = url.strip()
        self.group = group
        self.logo = logo
        self.hash = hashlib.md5(f"{self.name}{self.url}".encode()).hexdigest()
    
    def __eq__(self, other):
        return self.hash == other.hash
    
    def __hash__(self):
        return int(self.hash, 16)
    
    def to_m3u_line(self):
        """è½¬æ¢ä¸ºM3Uæ ¼å¼è¡Œ"""
        line = f'#EXTINF:-1'
        if self.group:
            line += f' group-title="{self.group}"'
        if self.logo:
            line += f' tvg-logo="{self.logo}"'
        line += f',{self.name}\n{self.url}\n'
        return line

class SourceCollector:
    """æºæ”¶é›†å™¨"""
    
    def __init__(self):
        self.all_channels = set()
        self.collected_count = 0
        self.processed_count = 0
        
    def fetch_source(self, url):
        """è·å–å•ä¸ªæº"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"âŒ è·å–æºå¤±è´¥ {url}: {e}")
            return None
    
    def parse_m3u(self, content, source_name):
        """è§£æM3Uå†…å®¹"""
        channels = []
        lines = content.split('\n')
        i = 0
        
        while i < len(lines):
            if lines[i].startswith('#EXTINF'):
                # è§£æEXTINFè¡Œ
                extinf = lines[i]
                channel_name = self.extract_channel_name(extinf)
                group = self.extract_group(extinf)
                logo = self.extract_logo(extinf)
                
                # è·å–URL
                if i + 1 < len(lines) and lines[i + 1].strip() and not lines[i + 1].startswith('#'):
                    url = lines[i + 1].strip()
                    if url and self.is_valid_url(url):
                        channel = Channel(channel_name, url, group, logo)
                        channel.source = source_name
                        channels.append(channel)
                        i += 1
            i += 1
        
        return channels
    
    def extract_channel_name(self, extinf_line):
        """ä»EXTINFè¡Œæå–é¢‘é“åç§°"""
        # åŒ¹é…æ ¼å¼: #EXTINF:-1 tvg-id="" tvg-name="CCTV1" tvg-logo="" group-title="å¤®è§†",CCTV1
        match = re.search(r',([^,\n]+)$', extinf_line)
        if match:
            return match.group(1).strip()
        
        # å°è¯•ä»tvg-nameæå–
        match = re.search(r'tvg-name="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        
        return "æœªçŸ¥é¢‘é“"
    
    def extract_group(self, extinf_line):
        """ä»EXTINFè¡Œæå–åˆ†ç»„"""
        match = re.search(r'group-title="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        return None
    
    def extract_logo(self, extinf_line):
        """ä»EXTINFè¡Œæå–logo"""
        match = re.search(r'tvg-logo="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        return None
    
    def is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        patterns = [
            r'^https?://',
            r'^rtmp://',
            r'^rtsp://',
            r'^udp://',
            r'^http-flv://',
            r'^webrtc://'
        ]
        for pattern in patterns:
            if re.match(pattern, url):
                return True
        return False
    
    def categorize_channel(self, channel_name):
        """æ ¹æ®è§„åˆ™å½’ç±»é¢‘é“"""
        for category, patterns in CATEGORY_RULES.items():
            for pattern in patterns:
                if re.search(pattern, channel_name, re.IGNORECASE):
                    return category
        
        # é»˜è®¤åˆ†ç±»
        if any(keyword in channel_name for keyword in ['æµ‹è¯•', 'Test']):
            return 'æµ‹è¯•'
        return 'å…¶ä»–'
    
    def collect_all_sources(self):
        """æ”¶é›†æ‰€æœ‰æº"""
        print("ğŸš€ å¼€å§‹é‡‡é›†ç›´æ’­æº...")
        
        # ä»æ–‡ä»¶è¯»å–é¢å¤–æº
        if Path(ADDITIONAL_SOURCES_FILE).exists():
            with open(ADDITIONAL_SOURCES_FILE, 'r', encoding='utf-8') as f:
                additional_sources = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                SOURCES.extend(additional_sources)
        
        all_channels = set()
        
        for source_url in SOURCES:
            print(f"ğŸ“¡ æ­£åœ¨å¤„ç†: {source_url}")
            content = self.fetch_source(source_url)
            if content:
                channels = self.parse_m3u(content, source_url)
                new_count = len(channels)
                self.collected_count += new_count
                all_channels.update(channels)
                print(f"   âœ… é‡‡é›†åˆ° {new_count} ä¸ªé¢‘é“")
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        self.all_channels = all_channels
        self.processed_count = len(all_channels)
        print(f"\nğŸ“Š é‡‡é›†å®Œæˆï¼")
        print(f"   å…±é‡‡é›†: {self.collected_count} ä¸ªé¢‘é“")
        print(f"   å»é‡å: {self.processed_count} ä¸ªé¢‘é“")
    
    def generate_m3u_file(self):
        """ç”ŸæˆM3Uæ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æŒ‰åˆ†ç±»ç»„ç»‡é¢‘é“
        categorized = defaultdict(list)
        for channel in self.all_channels:
            category = self.categorize_channel(channel.name)
            # æ›´æ–°åˆ†ç»„ä¿¡æ¯
            channel.group = category
            categorized[category].append(channel)
        
        # ç”Ÿæˆå®Œæ•´çš„M3Uæ–‡ä»¶
        with open('live_sources.m3u', 'w', encoding='utf-8') as f:
            f.write('#EXTM3U x-tvg-url=""\n')
            f.write(f'# Generated by GitHub Actions at {timestamp}\n')
            f.write(f'# Total Channels: {self.processed_count}\n')
            f.write(f'# Sources: {len(SOURCES)}\n\n')
            
            # æŒ‰åˆ†ç±»å†™å…¥
            for category in sorted(categorized.keys()):
                channels = sorted(categorized[category], key=lambda x: x.name)
                f.write(f'\n# åˆ†ç±»: {category} ({len(channels)}ä¸ªé¢‘é“)\n')
                for channel in channels:
                    f.write(channel.to_m3u_line())
        
        # ç”ŸæˆæŒ‰åˆ†ç±»çš„æ–‡ä»¶
        Path('categories').mkdir(exist_ok=True)
        for category, channels in categorized.items():
            channels = sorted(channels, key=lambda x: x.name)
            with open(f'categories/{category}.m3u', 'w', encoding='utf-8') as f:
                f.write('#EXTM3U\n')
                f.write(f'# åˆ†ç±»: {category} ({len(channels)}ä¸ªé¢‘é“)\n')
                f.write(f'# æ›´æ–°æ—¶é—´: {timestamp}\n\n')
                for channel in channels:
                    f.write(channel.to_m3u_line())
        
        # ç”Ÿæˆé¢‘é“åˆ—è¡¨JSONï¼ˆç”¨äºç½‘é¡µå±•ç¤ºï¼‰
        channel_list = []
        for channel in sorted(self.all_channels, key=lambda x: x.name):
            channel_list.append({
                'name': channel.name,
                'url': channel.url,
                'category': channel.group,
                'logo': channel.logo
            })
        
        with open('channels.json', 'w', encoding='utf-8') as f:
            json.dump({
                'last_updated': timestamp,
                'total_channels': self.processed_count,
                'channels': channel_list
            }, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆREADMEç»Ÿè®¡ä¿¡æ¯
        self.generate_readme(categorized, timestamp)
        
        return True
    
    def generate_readme(self, categorized, timestamp):
        """ç”ŸæˆREADMEæ–‡ä»¶"""
        readme_content = f"""# ğŸ“º ç”µè§†ç›´æ’­æºæ”¶é›†é¡¹ç›®

è‡ªåŠ¨æ”¶é›†æ•´ç†çš„ç”µè§†ç›´æ’­æºï¼Œæ¯æ—¥è‡ªåŠ¨æ›´æ–°ã€‚

## ğŸ“Š ç»Ÿè®¡æ•°æ®
- **æœ€åæ›´æ–°**: {timestamp}
- **é¢‘é“æ€»æ•°**: {self.processed_count}
- **æ•°æ®æº**: {len(SOURCES)} ä¸ª

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶å | æè¿° |
|--------|------|
| `live_sources.m3u` | å®Œæ•´çš„ç›´æ’­æºæ–‡ä»¶ï¼ˆæ‰€æœ‰é¢‘é“ï¼‰ |
| `channels.json` | é¢‘é“ä¿¡æ¯JSONæ ¼å¼ |
| `categories/` | æŒ‰åˆ†ç±»åˆ†å¼€çš„M3Uæ–‡ä»¶ç›®å½• |
| `sources.txt` | è‡ªå®šä¹‰æºåˆ—è¡¨ï¼ˆä¸€è¡Œä¸€ä¸ªURLï¼‰ |

## ğŸ“‚ é¢‘é“åˆ†ç±»

"""
        for category in sorted(categorized.keys()):
            count = len(categorized[category])
            readme_content += f"- **{category}**: {count} ä¸ªé¢‘é“\n"

        readme_content += """

## ğŸ“¡ æ•°æ®æº

"""
        for source in SOURCES:
            readme_content += f"- {source}\n"

        readme_content += """

## ğŸ”§ ä½¿ç”¨è¯´æ˜

1. **ç›´æ¥ä½¿ç”¨**: ä¸‹è½½ `live_sources.m3u` æ–‡ä»¶ï¼Œåœ¨æ”¯æŒM3Uæ ¼å¼çš„æ’­æ”¾å™¨ä¸­æ‰“å¼€
2. **æŒ‰åˆ†ç±»ä½¿ç”¨**: ä¸‹è½½ `categories/` ç›®å½•ä¸‹å¯¹åº”åˆ†ç±»çš„æ–‡ä»¶
3. **æ·»åŠ è‡ªå®šä¹‰æº**: ç¼–è¾‘ `sources.txt` æ–‡ä»¶ï¼Œæ¯è¡Œæ·»åŠ ä¸€ä¸ªM3UæºURL

## âš™ï¸ è‡ªåŠ¨æ›´æ–°

æœ¬é¡¹ç›®ä½¿ç”¨ GitHub Actions è‡ªåŠ¨æ›´æ–°ï¼š
- æ¯å¤© UTC æ—¶é—´ 18:00ï¼ˆåŒ—äº¬æ—¶é—´å‡Œæ™¨2ç‚¹ï¼‰è‡ªåŠ¨è¿è¡Œ
- æ‰‹åŠ¨è§¦å‘ï¼šåœ¨ GitHub Actions é¡µé¢ç‚¹å‡» "Run workflow"
- ä¿®æ”¹ `sources.txt` åè‡ªåŠ¨è§¦å‘

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®æ”¶é›†çš„ç›´æ’­æºæ¥è‡ªç½‘ç»œï¼Œä»…ä¾›å­¦ä¹ å’Œæµ‹è¯•ä½¿ç”¨ã€‚
"""

        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)

def main():
    """ä¸»å‡½æ•°"""
    collector = SourceCollector()
    collector.collect_all_sources()
    
    if collector.processed_count > 0:
        collector.generate_m3u_file()
        print("\nâœ… æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
        print(f"   å®Œæ•´æ–‡ä»¶: live_sources.m3u")
        print(f"   åˆ†ç±»æ–‡ä»¶: categories/")
        print(f"   JSONæ•°æ®: channels.json")
        print(f"   ç»Ÿè®¡ä¿¡æ¯: README.md")
        
        # è®¾ç½®GitHub Actionsè¾“å‡º
        print(f"::set-output name=changed::true")
        print(f"::set-output name=channels::{collector.processed_count}")
    else:
        print("âŒ æœªé‡‡é›†åˆ°ä»»ä½•é¢‘é“")
        print(f"::set-output name=changed::false")

if __name__ == "__main__":
    main()