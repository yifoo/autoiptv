#!/usr/bin/env python3
"""
è‡ªåŠ¨é‡‡é›†å¹¶å½’ç±»ç”µè§†ç›´æ’­æº
"""

import requests
import re
import time
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import hashlib
import json
import os
import sys

# è¦é‡‡é›†çš„æºåˆ—è¡¨
SOURCES = [
    "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/ipv6.m3u",
    "https://raw.githubusercontent.com/chao921125/source/refs/heads/main/iptv/index.m3u"
]

# å¯ä»¥æ·»åŠ æ›´å¤šæº
ADDITIONAL_SOURCES_FILE = "sources.txt"

# åˆ†ç±»è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é¢‘é“åç§°ï¼‰
CATEGORY_RULES = {
    "å¤®è§†": [r"CCTV", r"å¤®è§†"],
    "å«è§†": [r"å«è§†"],
    "åœ°æ–¹å°": [r"åœ°æ–¹", r"éƒ½å¸‚", r"æ°‘ç”Ÿ", r"æ–°é—»"],
    "æ¸¯æ¾³å°": [r"å‡¤å‡°", r"ç¿¡ç¿ ", r"æ˜ç ", r"TVB", r"é¦™æ¸¯", r"å°æ¹¾"],
    "ä½“è‚²": [r"ä½“è‚²", r"NBA", r"è¶³çƒ", r"ç¯®çƒ"],
    "ç”µå½±": [r"ç”µå½±", r"å½±é™¢"],
    "éŸ³ä¹": [r"éŸ³ä¹", r"MTV"]
}

class Channel:
    """é¢‘é“ç±»"""
    def __init__(self, name, url, group=None, logo=None):
        self.name = name.strip()
        self.url = url.strip()
        self.group = group
        self.logo = logo
        self.hash = hashlib.md5(f"{self.name}{self.url}".encode()).hexdigest()
    
    def __eq__(self, other):
        return self.hash == other.hash
    
    def __hash__(self):
        return int(self.hash, 16)
    
    def to_m3u_line(self):
        """è½¬æ¢ä¸ºM3Uæ ¼å¼è¡Œ"""
        line = f'#EXTINF:-1'
        if self.group:
            line += f' group-title="{self.group}"'
        if self.logo:
            line += f' tvg-logo="{self.logo}"'
        line += f',{self.name}\n{self.url}\n'
        return line

class SourceCollector:
    """æºæ”¶é›†å™¨"""
    
    def __init__(self):
        self.all_channels = set()
        self.collected_count = 0
        self.processed_count = 0
        
    def fetch_source(self, url):
        """è·å–å•ä¸ªæº"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"âŒ è·å–æºå¤±è´¥ {url}: {e}")
            return None
    
    def parse_m3u(self, content):
        """è§£æM3Uå†…å®¹"""
        channels = []
        lines = content.split('\n')
        i = 0
        
        while i < len(lines):
            if lines[i].startswith('#EXTINF'):
                extinf = lines[i]
                channel_name = self.extract_channel_name(extinf)
                group = self.extract_group(extinf)
                logo = self.extract_logo(extinf)
                
                if i + 1 < len(lines) and lines[i + 1].strip() and not lines[i + 1].startswith('#'):
                    url = lines[i + 1].strip()
                    if url and self.is_valid_url(url):
                        channel = Channel(channel_name, url, group, logo)
                        channels.append(channel)
                        i += 1
            i += 1
        
        return channels
    
    def extract_channel_name(self, extinf_line):
        """ä»EXTINFè¡Œæå–é¢‘é“åç§°"""
        # åŒ¹é…æ ¼å¼: #EXTINF:-1,Channel Name
        match = re.search(r',([^,\n]+)$', extinf_line)
        if match:
            return match.group(1).strip()
        
        match = re.search(r'tvg-name="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        
        return "æœªçŸ¥é¢‘é“"
    
    def extract_group(self, extinf_line):
        """ä»EXTINFè¡Œæå–åˆ†ç»„"""
        match = re.search(r'group-title="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        return None
    
    def extract_logo(self, extinf_line):
        """ä»EXTINFè¡Œæå–logo"""
        match = re.search(r'tvg-logo="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        return None
    
    def is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or url.startswith('#'):
            return False
            
        patterns = [r'^https?://', r'^rtmp://', r'^rtsp://']
        for pattern in patterns:
            if re.match(pattern, url, re.IGNORECASE):
                return True
        return False
    
    def categorize_channel(self, channel_name):
        """æ ¹æ®è§„åˆ™å½’ç±»é¢‘é“"""
        channel_name_lower = channel_name.lower()
        
        for category, patterns in CATEGORY_RULES.items():
            for pattern in patterns:
                if re.search(pattern, channel_name, re.IGNORECASE):
                    return category
        
        return 'å…¶ä»–'
    
    def collect_all_sources(self):
        """æ”¶é›†æ‰€æœ‰æº"""
        print("ğŸš€ å¼€å§‹é‡‡é›†ç›´æ’­æº...")
        
        # ä»æ–‡ä»¶è¯»å–é¢å¤–æº
        if Path(ADDITIONAL_SOURCES_FILE).exists():
            with open(ADDITIONAL_SOURCES_FILE, 'r', encoding='utf-8') as f:
                additional_sources = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                SOURCES.extend(additional_sources)
        
        all_channels = set()
        
        for idx, source_url in enumerate(SOURCES, 1):
            print(f"\nğŸ“¡ [{idx}/{len(SOURCES)}] æ­£åœ¨å¤„ç†: {source_url}")
            content = self.fetch_source(source_url)
            if content:
                channels = self.parse_m3u(content)
                new_count = len(channels)
                self.collected_count += new_count
                before_merge = len(all_channels)
                all_channels.update(channels)
                after_merge = len(all_channels)
                added = after_merge - before_merge
                print(f"   âœ… é‡‡é›†åˆ° {new_count} ä¸ªé¢‘é“ï¼Œæ–°å¢ {added} ä¸ª")
            else:
                print(f"   âŒ è·å–å¤±è´¥")
            
            if idx < len(SOURCES):
                time.sleep(1)
        
        self.all_channels = all_channels
        self.processed_count = len(all_channels)
        print(f"\nğŸ“Š é‡‡é›†å®Œæˆï¼")
        print(f"   å…±é‡‡é›†: {self.collected_count} ä¸ªé¢‘é“")
        print(f"   å»é‡å: {self.processed_count} ä¸ªé¢‘é“")
        
        return len(all_channels) > 0
    
    def generate_output_files(self):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æŒ‰åˆ†ç±»ç»„ç»‡é¢‘é“
        categorized = defaultdict(list)
        for channel in self.all_channels:
            category = self.categorize_channel(channel.name)
            channel.group = category
            categorized[category].append(channel)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path('categories').mkdir(exist_ok=True)
        
        # ç”Ÿæˆå®Œæ•´çš„M3Uæ–‡ä»¶
        self.generate_m3u_file(categorized, timestamp)
        
        # ç”Ÿæˆåˆ†ç±»æ–‡ä»¶
        self.generate_category_files(categorized, timestamp)
        
        # ç”ŸæˆJSONæ–‡ä»¶
        self.generate_json_file(categorized, timestamp)
        
        # ç”ŸæˆREADME
        self.generate_readme(categorized, timestamp)
        
        # ç”Ÿæˆç®€å•çš„index.html
        self.generate_simple_html(categorized, timestamp)
        
        return True
    
    def generate_m3u_file(self, categorized, timestamp):
        """ç”Ÿæˆå®Œæ•´çš„M3Uæ–‡ä»¶"""
        with open('live_sources.m3u', 'w', encoding='utf-8') as f:
            f.write('#EXTM3U\n')
            f.write(f'# Generated at {timestamp}\n')
            f.write(f'# Total Channels: {self.processed_count}\n\n')
            
            for category in sorted(categorized.keys()):
                channels = sorted(categorized[category], key=lambda x: x.name)
                f.write(f'# åˆ†ç±»: {category} ({len(channels)}ä¸ªé¢‘é“)\n')
                for channel in channels:
                    f.write(channel.to_m3u_line())
    
    def generate_category_files(self, categorized, timestamp):
        """ç”Ÿæˆåˆ†ç±»M3Uæ–‡ä»¶"""
        for category, channels in categorized.items():
            channels = sorted(channels, key=lambda x: x.name)
            with open(f'categories/{category}.m3u', 'w', encoding='utf-8') as f:
                f.write('#EXTM3U\n')
                f.write(f'# åˆ†ç±»: {category} ({len(channels)}ä¸ªé¢‘é“)\n')
                f.write(f'# æ›´æ–°æ—¶é—´: {timestamp}\n\n')
                for channel in channels:
                    f.write(channel.to_m3u_line())
    
    def generate_json_file(self, categorized, timestamp):
        """ç”ŸæˆJSONæ–‡ä»¶"""
        channel_list = []
        for channel in sorted(self.all_channels, key=lambda x: x.name):
            channel_list.append({
                'name': channel.name,
                'url': channel.url,
                'category': channel.group,
                'logo': channel.logo
            })
        
        with open('channels.json', 'w', encoding='utf-8') as f:
            json.dump({
                'last_updated': timestamp,
                'total_channels': self.processed_count,
                'sources_count': len(SOURCES),
                'channels': channel_list
            }, f, ensure_ascii=False, indent=2)
    
    def generate_readme(self, categorized, timestamp):
        """ç”ŸæˆREADMEæ–‡ä»¶"""
        readme_content = f"""# ğŸ“º ç”µè§†ç›´æ’­æºæ”¶é›†é¡¹ç›®

è‡ªåŠ¨æ”¶é›†æ•´ç†çš„ç”µè§†ç›´æ’­æºï¼Œæ¯æ—¥è‡ªåŠ¨æ›´æ–°ã€‚

## ğŸ“Š ç»Ÿè®¡æ•°æ®
- **æœ€åæ›´æ–°**: {timestamp}
- **é¢‘é“æ€»æ•°**: {self.processed_count}
- **æ•°æ®æº**: {len(SOURCES)} ä¸ª

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶å | æè¿° |
|--------|------|
| `live_sources.m3u` | å®Œæ•´çš„ç›´æ’­æºæ–‡ä»¶ï¼ˆæ‰€æœ‰é¢‘é“ï¼‰ |
| `channels.json` | é¢‘é“ä¿¡æ¯JSONæ ¼å¼ |
| `categories/` | æŒ‰åˆ†ç±»åˆ†å¼€çš„M3Uæ–‡ä»¶ç›®å½• |
| `sources.txt` | è‡ªå®šä¹‰æºåˆ—è¡¨ï¼ˆä¸€è¡Œä¸€ä¸ªURLï¼‰ |
| `index.html` | ç½‘é¡µæ’­æ”¾ç•Œé¢ |

## ğŸ“‚ é¢‘é“åˆ†ç±»

"""

        for category in sorted(categorized.keys()):
            count = len(categorized[category])
            readme_content += f"- **{category}**: {count} ä¸ªé¢‘é“\n"

        readme_content += """

## ğŸ“¡ æ•°æ®æº

"""

        for source in SOURCES:
            readme_content += f"- {source}\n"

        readme_content += """

## ğŸ”§ ä½¿ç”¨è¯´æ˜

1. **ç›´æ¥ä½¿ç”¨**: ä¸‹è½½ `live_sources.m3u` æ–‡ä»¶ï¼Œåœ¨æ”¯æŒM3Uæ ¼å¼çš„æ’­æ”¾å™¨ä¸­æ‰“å¼€
2. **æŒ‰åˆ†ç±»ä½¿ç”¨**: ä¸‹è½½ `categories/` ç›®å½•ä¸‹å¯¹åº”åˆ†ç±»çš„æ–‡ä»¶
3. **æ·»åŠ è‡ªå®šä¹‰æº**: ç¼–è¾‘ `sources.txt` æ–‡ä»¶ï¼Œæ¯è¡Œæ·»åŠ ä¸€ä¸ªM3UæºURL

## âš™ï¸ è‡ªåŠ¨æ›´æ–°

æœ¬é¡¹ç›®ä½¿ç”¨ GitHub Actions è‡ªåŠ¨æ›´æ–°ï¼š
- æ¯å¤© UTC æ—¶é—´ 18:00ï¼ˆåŒ—äº¬æ—¶é—´å‡Œæ™¨2ç‚¹ï¼‰è‡ªåŠ¨è¿è¡Œ
- æ‰‹åŠ¨è§¦å‘ï¼šåœ¨ GitHub Actions é¡µé¢ç‚¹å‡» "Run workflow"
- ä¿®æ”¹ `sources.txt` åè‡ªåŠ¨è§¦å‘

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®æ”¶é›†çš„ç›´æ’­æºæ¥è‡ªç½‘ç»œï¼Œä»…ä¾›å­¦ä¹ å’Œæµ‹è¯•ä½¿ç”¨ã€‚
"""

        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    def generate_simple_html(self, categorized, timestamp):
        """ç”Ÿæˆç®€å•çš„HTMLé¡µé¢"""
        html_content = '''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç”µè§†ç›´æ’­æºæ’­æ”¾åˆ—è¡¨</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        header { background: #4CAF50; color: white; padding: 20px; border-radius: 5px; }
        .stats { margin: 20px 0; }
        .category { margin: 20px 0; border: 1px solid #ddd; padding: 15px; border-radius: 5px; }
        .channel { margin: 5px 0; padding: 5px; background: #f5f5f5; border-radius: 3px; }
        .download { margin: 20px 0; }
        .btn { display: inline-block; background: #2196F3; color: white; padding: 10px 15px; text-decoration: none; border-radius: 5px; margin: 5px; }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“º ç”µè§†ç›´æ’­æºæ’­æ”¾åˆ—è¡¨</h1>
        <p>è‡ªåŠ¨æ”¶é›†æ•´ç†çš„ç”µè§†ç›´æ’­æº</p>
    </header>
    
    <div class="stats">
        <p><strong>æœ€åæ›´æ–°:</strong> ''' + timestamp + '''</p>
        <p><strong>é¢‘é“æ€»æ•°:</strong> ''' + str(self.processed_count) + '''</p>
        <p><strong>æ•°æ®æº:</strong> ''' + str(len(SOURCES)) + ''' ä¸ª</p>
    </div>
    
    <div class="download">
        <h3>ğŸ“¥ ä¸‹è½½æ’­æ”¾åˆ—è¡¨</h3>
        <a href="live_sources.m3u" class="btn">å®Œæ•´åˆ—è¡¨</a>
        <a href="channels.json" class="btn">JSON æ•°æ®</a>
'''
        
        for category in sorted(categorized.keys()):
            if len(categorized[category]) > 0:
                html_content += f'        <a href="categories/{category}.m3u" class="btn">{category}</a>\n'
        
        html_content += '''    </div>
    
    <h3>ğŸ“‚ é¢‘é“åˆ†ç±»</h3>
'''
        
        for category in sorted(categorized.keys()):
            channels = categorized[category]
            html_content += f'''    <div class="category">
        <h4>{category} ({len(channels)}ä¸ªé¢‘é“)</h4>
'''
            
            for channel in sorted(channels[:10], key=lambda x: x.name):  # åªæ˜¾ç¤ºå‰10ä¸ª
                html_content += f'''        <div class="channel">
            {channel.name}
            <button onclick="window.open('{channel.url}', '_blank')">æ’­æ”¾</button>
        </div>
'''
            
            if len(channels) > 10:
                html_content += f'''        <p>... è¿˜æœ‰ {len(channels) - 10} ä¸ªé¢‘é“</p>
'''
            
            html_content += '''    </div>
'''
        
        html_content += '''    
    <footer style="margin-top: 30px; padding-top: 20px; border-top: 1px solid #ddd;">
        <p>è‡ªåŠ¨æ›´æ–°äº GitHub Actions | æœ€åæ›´æ–°: ''' + timestamp + '''</p>
    </footer>
    
    <script>
        function copyUrl(url) {
            navigator.clipboard.writeText(url).then(() => {
                alert('URLå·²å¤åˆ¶åˆ°å‰ªè´´æ¿');
            });
        }
    </script>
</body>
</html>'''
        
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(html_content)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç”µè§†ç›´æ’­æºè‡ªåŠ¨é‡‡é›†å·¥å…·")
    print("=" * 60)
    
    collector = SourceCollector()
    
    try:
        has_data = collector.collect_all_sources()
        
        if has_data and collector.processed_count > 0:
            collector.generate_output_files()
            print("\nâœ… æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
            print(f"   å®Œæ•´æ–‡ä»¶: live_sources.m3u")
            print(f"   åˆ†ç±»æ–‡ä»¶: categories/*.m3u")
            print(f"   JSONæ•°æ®: channels.json")
            print(f"   ç½‘é¡µç•Œé¢: index.html")
            print(f"   ç»Ÿè®¡ä¿¡æ¯: README.md")
            print(f"\nğŸ‰ é‡‡é›†æˆåŠŸï¼å…±å¤„ç† {collector.processed_count} ä¸ªé¢‘é“")
            
            # è®¾ç½®GitHub Actionsè¾“å‡º
            with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                print(f'changed=true', file=fh)
                print(f'channels={collector.processed_count}', file=fh)
        else:
            print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•æœ‰æ•ˆé¢‘é“")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                print(f'changed=false', file=fh)
    
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
            print(f'changed=false', file=fh)
        sys.exit(1)

if __name__ == "__main__":
    main()