#!/usr/bin/env python3
"""
è‡ªåŠ¨é‡‡é›†å¹¶å½’ç±»ç”µè§†ç›´æ’­æº
"""

import requests
import re
import time
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import hashlib
import json
import os
import sys

# è¦é‡‡é›†çš„æºåˆ—è¡¨
SOURCES = [
    "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/ipv6.m3u",
    "https://raw.githubusercontent.com/chao921125/source/refs/heads/main/iptv/index.m3u"
]

# å¯ä»¥æ·»åŠ æ›´å¤šæº
ADDITIONAL_SOURCES_FILE = "sources.txt"

# åˆ†ç±»è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é¢‘é“åç§°ï¼‰
CATEGORY_RULES = {
    "å¤®è§†": [
        r"CCTV[-_\s]?\d+", r"CCTV[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+",
        r"å¤®è§†[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+", r"ä¸­å¤®ç”µè§†å°", r"CCTV1", r"CCTV2"
    ],
    "å«è§†": [
        r"å«è§†", r"æ¹–å—å«è§†", r"æµ™æ±Ÿå«è§†", r"æ±Ÿè‹å«è§†", r"ä¸œæ–¹å«è§†",
        r"åŒ—äº¬å«è§†", r"å¤©æ´¥å«è§†", r"å®‰å¾½å«è§†", r"å±±ä¸œå«è§†", r"å¹¿ä¸œå«è§†",
        r"æ·±åœ³å«è§†", r"é»‘é¾™æ±Ÿå«è§†", r"è¾½å®å«è§†", r"æ¹–åŒ—å«è§†", r"æ²³å—å«è§†"
    ],
    "åœ°æ–¹å°": [
        r"åœ°æ–¹", r"éƒ½å¸‚", r"æ°‘ç”Ÿ", r"æ–°é—»", r"å…¬å…±", r"ç”Ÿæ´»",
        r"æ•™è‚²", r"å°‘å„¿", r"ç»¼è‰º", r"ç»æµ", r"æ³•åˆ¶", r"å†œä¸š"
    ],
    "æ¸¯æ¾³å°": [
        r"å‡¤å‡°", r"ç¿¡ç¿ ", r"æ˜ç ", r"TVB", r"ATV", r"æ¾³è§†",
        r"æ¾³é—¨", r"é¦™æ¸¯", r"å°æ¹¾", r"ä¸­å¤©", r"ä¸œæ£®", r"åè§†",
        r"æ°‘è§†", r"ä¸‰ç«‹", r"æ— çº¿"
    ],
    "ä½“è‚²": [
        r"ä½“è‚²", r"NBA", r"è¶³çƒ", r"ç¯®çƒ", r"é«˜å°”å¤«", r"ç½‘çƒ",
        r"ä¹’ç¾½", r"æå‡»", r"èµ›è½¦", r"å¥¥è¿"
    ],
    "ç”µå½±": [
        r"ç”µå½±", r"å½±é™¢", r"å½±è§†é¢‘é“", r"å¥½è±å", r"CHC"
    ],
    "éŸ³ä¹": [
        r"éŸ³ä¹", r"MTV", r"æ¼”å”±ä¼š", r"Kæ­Œ", r"æˆæ›²"
    ],
    "å›½é™…": [
        r"BBC", r"CNN", r"NHK", r"DW", r"æ³•å›½", r"å¾·å›½",
        r"ä¿„ç½—æ–¯", r"éŸ©å›½", r"æ—¥æœ¬", r"ç¾å›½"
    ]
}

class Channel:
    """é¢‘é“ç±»"""
    def __init__(self, name, url, group=None, logo=None):
        self.name = name.strip()
        self.url = url.strip()
        self.group = group
        self.logo = logo
        self.hash = hashlib.md5(f"{self.name}{self.url}".encode()).hexdigest()
    
    def __eq__(self, other):
        return self.hash == other.hash
    
    def __hash__(self):
        return int(self.hash, 16)
    
    def to_m3u_line(self):
        """è½¬æ¢ä¸ºM3Uæ ¼å¼è¡Œ"""
        line = f'#EXTINF:-1'
        if self.group:
            line += f' group-title="{self.group}"'
        if self.logo:
            line += f' tvg-logo="{self.logo}"'
        line += f',{self.name}\n{self.url}\n'
        return line

class SourceCollector:
    """æºæ”¶é›†å™¨"""
    
    def __init__(self):
        self.all_channels = set()
        self.collected_count = 0
        self.processed_count = 0
        
    def fetch_source(self, url):
        """è·å–å•ä¸ªæº"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"âŒ è·å–æºå¤±è´¥ {url}: {e}")
            return None
    
    def parse_m3u(self, content, source_name):
        """è§£æM3Uå†…å®¹"""
        channels = []
        lines = content.split('\n')
        i = 0
        
        while i < len(lines):
            if lines[i].startswith('#EXTINF'):
                # è§£æEXTINFè¡Œ
                extinf = lines[i]
                channel_name = self.extract_channel_name(extinf)
                group = self.extract_group(extinf)
                logo = self.extract_logo(extinf)
                
                # è·å–URL
                if i + 1 < len(lines) and lines[i + 1].strip() and not lines[i + 1].startswith('#'):
                    url = lines[i + 1].strip()
                    if url and self.is_valid_url(url):
                        channel = Channel(channel_name, url, group, logo)
                        channels.append(channel)
                        i += 1
            i += 1
        
        return channels
    
    def extract_channel_name(self, extinf_line):
        """ä»EXTINFè¡Œæå–é¢‘é“åç§°"""
        # åŒ¹é…æ ¼å¼: #EXTINF:-1,Channel Name
        match = re.search(r',([^,\n]+)$', extinf_line)
        if match:
            return match.group(1).strip()
        
        # å°è¯•ä»tvg-nameæå–
        match = re.search(r'tvg-name="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        
        # æœ€åå°è¯•æå–é¢‘é“å
        match = re.search(r',([^,]+)$', extinf_line)
        if match:
            return match.group(1).strip()
        
        return "æœªçŸ¥é¢‘é“"
    
    def extract_group(self, extinf_line):
        """ä»EXTINFè¡Œæå–åˆ†ç»„"""
        match = re.search(r'group-title="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        return None
    
    def extract_logo(self, extinf_line):
        """ä»EXTINFè¡Œæå–logo"""
        match = re.search(r'tvg-logo="([^"]+)"', extinf_line)
        if match:
            return match.group(1).strip()
        return None
    
    def is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or url.startswith('#'):
            return False
            
        patterns = [
            r'^https?://',
            r'^rtmp://',
            r'^rtsp://',
            r'^udp://',
            r'^http-flv://',
            r'^webrtc://'
        ]
        for pattern in patterns:
            if re.match(pattern, url, re.IGNORECASE):
                return True
        return False
    
    def categorize_channel(self, channel_name):
        """æ ¹æ®è§„åˆ™å½’ç±»é¢‘é“"""
        channel_name_lower = channel_name.lower()
        
        for category, patterns in CATEGORY_RULES.items():
            for pattern in patterns:
                if re.search(pattern, channel_name, re.IGNORECASE):
                    return category
        
        # é»˜è®¤åˆ†ç±»
        if any(keyword in channel_name_lower for keyword in ['æµ‹è¯•', 'test']):
            return 'æµ‹è¯•'
        return 'å…¶ä»–'
    
    def collect_all_sources(self):
        """æ”¶é›†æ‰€æœ‰æº"""
        print("ğŸš€ å¼€å§‹é‡‡é›†ç›´æ’­æº...")
        
        # ä»æ–‡ä»¶è¯»å–é¢å¤–æº
        if Path(ADDITIONAL_SOURCES_FILE).exists():
            with open(ADDITIONAL_SOURCES_FILE, 'r', encoding='utf-8') as f:
                additional_sources = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                SOURCES.extend(additional_sources)
        
        all_channels = set()
        
        for idx, source_url in enumerate(SOURCES, 1):
            print(f"\nğŸ“¡ [{idx}/{len(SOURCES)}] æ­£åœ¨å¤„ç†: {source_url}")
            content = self.fetch_source(source_url)
            if content:
                channels = self.parse_m3u(content, source_url)
                new_count = len(channels)
                self.collected_count += new_count
                before_merge = len(all_channels)
                all_channels.update(channels)
                after_merge = len(all_channels)
                added = after_merge - before_merge
                print(f"   âœ… é‡‡é›†åˆ° {new_count} ä¸ªé¢‘é“ï¼Œæ–°å¢ {added} ä¸ª")
            else:
                print(f"   âŒ è·å–å¤±è´¥")
            
            if idx < len(SOURCES):
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        self.all_channels = all_channels
        self.processed_count = len(all_channels)
        print(f"\nğŸ“Š é‡‡é›†å®Œæˆï¼")
        print(f"   å…±é‡‡é›†: {self.collected_count} ä¸ªé¢‘é“")
        print(f"   å»é‡å: {self.processed_count} ä¸ªé¢‘é“")
        
        return len(all_channels) > 0
    
    def generate_output_files(self):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æŒ‰åˆ†ç±»ç»„ç»‡é¢‘é“
        categorized = defaultdict(list)
        for channel in self.all_channels:
            category = self.categorize_channel(channel.name)
            # æ›´æ–°åˆ†ç»„ä¿¡æ¯
            channel.group = category
            categorized[category].append(channel)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path('categories').mkdir(exist_ok=True)
        
        # ç”Ÿæˆå®Œæ•´çš„M3Uæ–‡ä»¶
        self.generate_m3u_file(categorized, timestamp)
        
        # ç”Ÿæˆåˆ†ç±»æ–‡ä»¶
        self.generate_category_files(categorized, timestamp)
        
        # ç”ŸæˆJSONæ–‡ä»¶
        self.generate_json_file(categorized, timestamp)
        
        # ç”ŸæˆREADME
        self.generate_readme(categorized, timestamp)
        
        # ç”ŸæˆHTMLé¡µé¢
        self.generate_html_file(categorized, timestamp)
        
        return True
    
    def generate_m3u_file(self, categorized, timestamp):
        """ç”Ÿæˆå®Œæ•´çš„M3Uæ–‡ä»¶"""
        with open('live_sources.m3u', 'w', encoding='utf-8') as f:
            f.write('#EXTM3U x-tvg-url=""\n')
            f.write(f'# Generated by GitHub Actions at {timestamp}\n')
            f.write(f'# Total Channels: {self.processed_count}\n')
            f.write(f'# Sources: {len(SOURCES)}\n\n')
            
            # æŒ‰åˆ†ç±»å†™å…¥
            for category in sorted(categorized.keys()):
                channels = sorted(categorized[category], key=lambda x: x.name)
                f.write(f'\n# åˆ†ç±»: {category} ({len(channels)}ä¸ªé¢‘é“)\n')
                for channel in channels:
                    f.write(channel.to_m3u_line())
    
    def generate_category_files(self, categorized, timestamp):
        """ç”Ÿæˆåˆ†ç±»M3Uæ–‡ä»¶"""
        for category, channels in categorized.items():
            channels = sorted(channels, key=lambda x: x.name)
            with open(f'categories/{category}.m3u', 'w', encoding='utf-8') as f:
                f