#!/usr/bin/env python3
"""
ä¸»æ”¶é›†æ¨¡å—
"""
import os
import sys
import time
from pathlib import Path
from .utils import fetch_m3u, get_beijing_time
from .channel_processor import parse_channels, merge_channels, categorize_channel
from .speed_tester import test_urls_with_progress
from .blacklist_manager import load_blacklist, save_to_blacklist


def load_sources_from_file():
    """ä»sources.txtæ–‡ä»¶åŠ è½½æ‰€æœ‰ç”µè§†æº"""
    sources_file = "sources.txt"
    sources = []
    
    if not os.path.exists(sources_file):
        print(f"âŒ é”™è¯¯: {sources_file} æ–‡ä»¶ä¸å­˜åœ¨")
        print(f"ğŸ“ è¯·åˆ›å»º {sources_file} æ–‡ä»¶ï¼Œæ¯è¡Œæ·»åŠ ä¸€ä¸ªM3Uæ–‡ä»¶URL")
        return sources
    
    try:
        with open(sources_file, "r", encoding="utf-8") as f:
            line_number = 0
            for line in f:
                line_number += 1
                line = line.strip()
                
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                if not line:
                    continue
                if line.startswith("#"):
                    continue
                
                # éªŒè¯URLæ ¼å¼
                if line.startswith("http://") or line.startswith("https://"):
                    sources.append(line)
                else:
                    print(f"âš ï¸  ç¬¬{line_number}è¡Œæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡: {line}")
        
        print(f"ğŸ“¡ ä» {sources_file} åŠ è½½äº† {len(sources)} ä¸ªæ•°æ®æº")
        
        if len(sources) == 0:
            print(f"âŒ é”™è¯¯: {sources_file} ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
            print(f"ğŸ“ è¯·åœ¨ {sources_file} ä¸­æ·»åŠ M3Uæ–‡ä»¶URLï¼Œæ ¼å¼: https://example.com/live.m3u")
        
    except Exception as e:
        print(f"âŒ è¯»å– {sources_file} å¤±è´¥: {e}")
    
    return sources


def collect_and_process(sources):
    """ä¸»æ”¶é›†å’Œå¤„ç†å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é‡‡é›†ç”µè§†ç›´æ’­æº...")
    
    # 1. åŠ è½½é»‘åå•
    print("ğŸ“‹ åŠ è½½é»‘åå•...")
    blacklist = load_blacklist()
    
    print(f"ğŸ“‹ æ•°æ®æºåˆ—è¡¨ (ä»sources.txtåŠ è½½):")
    for i, source in enumerate(sources, 1):
        print(f"  {i:2d}. {source}")
    
    all_channels = []
    success_sources = 0
    failed_sources = []
    
    # 2. æ”¶é›†æ‰€æœ‰é¢‘é“çš„åŸå§‹æ•°æ®
    print("\nğŸ“¡ å¼€å§‹æ”¶é›†é¢‘é“æ•°æ®...")
    for idx, source_url in enumerate(sources, 1):
        print(f"\n[{idx}/{len(sources)}] å¤„ç†: {source_url}")
        
        content = fetch_m3u(source_url)
        if not content:
            failed_sources.append(source_url)
            print("   âŒ æ— æ³•è·å–å†…å®¹ï¼Œè·³è¿‡")
            continue
        
        channels = parse_channels(content, source_url)
        
        # ç»Ÿè®¡é¢‘é“åç§°å˜åŒ–
        changed_count = 0
        for channel in channels:
            if channel['original_name'] != channel['clean_name']:
                changed_count += 1
        
        print(f"   âœ… è§£æåˆ° {len(channels)} ä¸ªé¢‘é“ ({changed_count}ä¸ªå·²ç²¾ç®€)")
        
        if changed_count > 0 and len(channels) <= 10:
            for channel in channels[:5]:
                if channel['original_name'] != channel['clean_name']:
                    print(f"      '{channel['original_name']}' -> '{channel['clean_name']}'")
        
        all_channels.extend(channels)
        success_sources += 1
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        if idx < len(sources):
            time.sleep(1)
    
    print(f"\n{'='*50}")
    print(f"âœ… é‡‡é›†å®Œæˆç»Ÿè®¡:")
    print(f"   æˆåŠŸæºæ•°: {success_sources}/{len(sources)}")
    print(f"   å¤±è´¥æºæ•°: {len(failed_sources)}")
    print(f"   æ€»è®¡é‡‡é›†: {len(all_channels)} ä¸ªåŸå§‹é¢‘é“")
    
    if len(failed_sources) > 0:
        print(f"\nâš ï¸  å¤±è´¥çš„æº:")
        for failed in failed_sources:
            print(f"   - {failed}")
    
    if len(all_channels) == 0:
        print("\nâŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•é¢‘é“ï¼Œé€€å‡º")
        return None
    
    # 3. æå–æ‰€æœ‰å”¯ä¸€çš„URLè¿›è¡Œé€Ÿåº¦æµ‹è¯•
    print("\nğŸ“Š æå–æ‰€æœ‰å”¯ä¸€URL...")
    all_urls = set()
    for channel in all_channels:
        all_urls.add(channel['url'])
    
    print(f"   å‘ç° {len(all_urls)} ä¸ªå”¯ä¸€URL")
    
    # 4. è¿›è¡Œé€Ÿåº¦æµ‹è¯•
    print("\nâš¡ å¼€å§‹é€Ÿåº¦æµ‹è¯•ï¼ˆè¿‡æ»¤é»‘åå•ä¸­çš„URLï¼‰...")
    speed_test_results, slow_urls = test_urls_with_progress(all_urls, blacklist)
    
    # 5. ä¿å­˜æ–°çš„æ…¢é€ŸURLåˆ°é»‘åå•
    if slow_urls:
        print(f"\nğŸ“ å‘ç° {len(slow_urls)} ä¸ªæ…¢é€Ÿæºï¼Œä¿å­˜åˆ°é»‘åå•...")
        save_to_blacklist(slow_urls)
    else:
        print("\nâœ… æ²¡æœ‰å‘ç°æ–°çš„æ…¢é€Ÿæº")
    
    # 6. è¿‡æ»¤æ‰é»‘åå•ä¸­çš„é¢‘é“ï¼ˆåŒ…æ‹¬ä¹‹å‰é»‘åå•å’Œæœ¬æ¬¡å‘ç°çš„æ…¢é€Ÿæºï¼‰
    print("\nğŸš« è¿‡æ»¤é»‘åå•ä¸­çš„é¢‘é“...")
    filtered_channels = []
    blacklisted_count = 0
    
    for channel in all_channels:
        if channel['url'] in blacklist or channel['url'] in slow_urls:
            blacklisted_count += 1
        else:
            filtered_channels.append(channel)
    
    print(f"   åŸå§‹é¢‘é“æ•°: {len(all_channels)}")
    print(f"   è¿‡æ»¤åé¢‘é“æ•°: {len(filtered_channels)}")
    print(f"   é»‘åå•è¿‡æ»¤æ•°: {blacklisted_count}")
    
    if len(filtered_channels) == 0:
        print("\nâŒ æ‰€æœ‰é¢‘é“éƒ½è¢«é»‘åå•è¿‡æ»¤ï¼Œé€€å‡º")
        return None
    
    # 7. åˆå¹¶åŒåç”µè§†å°
    print("\nğŸ”„ æ­£åœ¨åˆå¹¶åŒåç”µè§†å°...")
    merged_channels = merge_channels(filtered_channels, speed_test_results)
    print(f"   åˆå¹¶å: {len(merged_channels)} ä¸ªå”¯ä¸€ç”µè§†å°")
    
    return {
        'merged_channels': merged_channels,
        'all_channels': all_channels,
        'filtered_channels': filtered_channels,
        'success_sources': success_sources,
        'failed_sources': failed_sources,
        'blacklisted_count': blacklisted_count,
        'speed_test_results': speed_test_results,
        'slow_urls': slow_urls,
        'blacklist': blacklist,
        'sources': sources
    }